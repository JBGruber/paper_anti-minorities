---
title: 1_get_data
author: Johannes B. Gruber
date: today
format:
  html:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    embed-resources: true
---

# Introduction

Our goal is to find out whether we can identify the weaponization of LGBTQ+ rights in party communication.
For this, we download data from the Guardian API.
We want articles that mention LGBTQ+ rights and a party or a politician affiliated with a party.

## Packages

I use httr2 to pull articles from the Guardian API.

```{r setup}
#| include: false
library(tidyverse)
library(furrr)
library(glue)
library(httr2)
start <- Sys.time() # note start time for later
```

## Functions

I write a couple of functions which effectively pull data from the Guardian API

```{r}
#| code-fold: true
# function to perform a search
# use an env to cache responses, if there is a need to interrupt the function
cache_env <- new.env()
search_guardian <- function(q, initial_page = 1L, max_pages = Inf, key) {
  # initial request
  resp <- base_req_search_guardian(q, p = initial_page, key = key[1]) |>
    req_perform()

  cache_env$resps <- list(cache_env$resps)
  info <- resp_body_json(resp) |>
    pluck("response")

  message("Found ", info$total, " articles to download")

  pgs <- seq_len(info$pages) |>
    tail(-initial_page) |>
    head(max_pages - 1L)

  req_list <- map(pgs, function(p) {
    base_req_search_guardian(q, p = p, key = sample(key, 1L))
  })

  cache_env$resps <- c(
    cache_env$resps,
    req_perform_sequential(req_list, on_error = "return", progress = interactive())
  )

  return(resps)
}


# helper function to construct base search request
base_req_search_guardian <- function(q, p, key) {
  request("https://content.guardianapis.com") |>
    req_url_path("search") |>
    req_timeout(seconds = 60) |>
    req_url_query(
      q = q,
      page = p,
      "show-blocks" = "all"
    ) |>
    req_url_query(
      "api-key" = key
    ) |>
    req_throttle(rate = 2L) |>
    req_retry(max_tries = 5L)
}


# parse a single response
parse_response <- function(resp) {
  dat <- resp_body_json(resp)
  res <- pluck(dat, "response", "results")
  if (!is.null(res)) {
    out <- map(res, parse_art_guardian) |>
      bind_rows()
    return(out)
  }

  res <- resp_body_json(resp) |>
    pluck("response", "docs")

  map(res, parse_art_guardian) |>
    bind_rows()
}

# helper function to parse a single article
parse_art_guardian <- function(a) {
  body <- pluck(a, "blocks", "body")
  text <- map_chr(body, function(b) {
    html <- pluck(b, "bodyHtml") |> 
      trimws()
    
    # several reasons for why this fails, E.g., videos and other formats have empty body
    text <- try({
      html |>
        rvest::read_html(options = "HUGE") |>
        rvest::html_text2()
    }, silent = TRUE)
    
    if (is(text, "try-error")) {
      text <- ""
    }
    return(text)
  }) |> 
    paste0(collapse = "\n\n")
  
  a$blocks <- NULL

  as_tibble(a) |>
    janitor::clean_names() |>
    mutate(
      web_publication_date = as_datetime(web_publication_date),
      text = text
    )
}

# put string in quotes if there is whitespace in it
in_quotes <- function(s) {
  ifelse(str_detect(s, "\\s"), glue("\"{s}\""), s)
}
```


# Main

To search the APIs, we need API keys:

```{r}
keys_guardian <- read_delim("keys_guardian.txt", delim = ",", col_names = "key", col_types = "c") |> 
  mutate(limit = FALSE)
```

I construct the search query by combining words describing the LGBTQ+ community and its members with the names of the parties in UK and US politics:

```{r}
lgbtq_terms <- readLines("lgbtq-dict.txt") |> 
  str_remove("#.*") |> 
  str_trim() |> 
  stringi::stri_remove_empty() |> 
  in_quotes() |> 
  paste(collapse = " OR ") 
```

```{r}
parties <- readLines("party-dict.txt") |> 
  str_remove("#.*") |> 
  str_trim() |> 
  stringi::stri_remove_empty() |> 
  in_quotes() |> 
  paste(collapse = " OR ")
```

We search for any articles containing at least one word from the LGBTQ+ set and one word from the party set:

```{r}
query <- glue("({lgbtq_terms}) AND ({parties})")
query |> 
  cat()
```

Now I can collect the data

```{r}
# since there is a rate limit on this, I cache the responses, in case I run the script again
dir.create("data", showWarnings = FALSE)
cache <- "data/resps_guardian.rds"
if (!file.exists(cache)) {
  resps_guardian <- search_guardian(q = query, initial_page = 1, key = pull(keys_guardian, key))
  saveRDS(resps_guardian, cache)
  # fails <- resps_failures(resps_guardian)
}
resps_guardian <- readRDS(cache)
```

I double-check if we really have all pages:

```{r}
pages <- resps_guardian |> 
  pluck(1) |> 
  resp_body_json() |> 
  pluck("response", "pages")

resps_guardian_rawdata <- resps_guardian |> 
  resps_successes() |> 
  resps_data(resp_body_json)

test_guardian <- tibble(page = map_int(resps_guardian_rawdata, "currentPage"),
                        done = TRUE) |> 
  right_join(tibble(page = seq_len(pages)), by = "page") 

test_guardian |> 
  count(done)
```

Now I can parse the responses into one data.frame:

```{r}
plan(multisession, workers = 6)
articles_guardian <- resps_guardian |> 
  future_map(parse_response, .progress = interactive()) |> 
  bind_rows()
```


## Some quick insights

How many articles do we have over time?

```{r}
#| label: fig-data-overview-full
#| fig-cap: "Overall number of articles over time (full)"
articles_guardian |> 
  count(date = lubridate::floor_date(web_publication_date, "month")) |> 
  ggplot(aes(x = date, y = n)) +
  geom_line() +
  labs(x = NULL, y = NULL) +
  theme_minimal()
```

Looks like there are some digitised articles from long ago.
But the Guardian online started publishing online in 1999.
We can disregard the spotty archive before that.

```{r}
#| label: fig-data-overview
#| fig-cap: "Overall number of articles over time"
articles_guardian |> 
  filter(web_publication_date > "1999-01-01") |> 
  count(date = lubridate::floor_date(web_publication_date, "month")) |> 
  ggplot(aes(x = date, y = n)) +
  geom_line() +
  labs(x = NULL, y = NULL) +
  theme_minimal()
```

Excluding the articles before 1999, we have 

```{r}
articles_guardian_clean <- articles_guardian |> 
  filter(web_publication_date > "1999-01-01") 

total <- nrow(articles_guardian_clean)
year_count <- articles_guardian_clean |> 
  count(year = year(web_publication_date))

year_max <- year_count |> 
  slice_max(n, n = 1)

year_min <- articles_guardian_clean |> 
  count(year = year(web_publication_date)) |> 
  slice_min(n, n = 1)

year_min <- year_count |> 
  slice_min(n, n = 1)

year_avg <- year_count |> 
  summarise(n = mean(n)) |> 
  pull(n) |> 
  as.integer()

tribble(
  ~"", ~Number,
  "total", scales::comma(total),
  "avg. per year", scales::comma(year_avg),
  "year max. (2016)",  scales::comma(year_max$n),
  "year min. (2016)",  scales::comma(year_min$n),
  "time frame", glue("{min(year_count$year)} -- {max(year_count$year)}")
)
```


# Wrap-up

```{r}
saveRDS(articles_guardian, "data/articles_guardian.rds")
```

Afterwards we get some information which is important to reproduce the report.

```{r}
sessionInfo()
Sys.time()
# note how long the script takes to (re-)run
Sys.time() - start
```
